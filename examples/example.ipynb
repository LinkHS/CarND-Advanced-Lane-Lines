{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n",
    "## Pipeline\n",
    "\n",
    "---\n",
    "### Camera calibration\n",
    "First, I'll compute the camera calibration using chessboard images. Then I will get the camera parameters which will be save to \"CamParams-0.pkl\" for later use. Using `CamParams.load()` function not only avoids repetitive calibration but also accelerates pre-process speed.\n",
    "\n",
    "Change the line `if 0:` to `if 1:` if you want switch to calibration mode from load mode.\n",
    "> `keep_global_var` and `printvars` is used to remove and show global defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "%reload_ext autoreload\n",
    "%aimport util\n",
    "%autoreload 1 \n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(2)\n",
    "\n",
    "def printvars():\n",
    "    tmp = globals().copy()\n",
    "    print('all defined vars:', end=' ') \n",
    "    [print(k, end=', ') for k, v in tmp.items() if not str(type(v)).endswith('module\\'>') \n",
    "                                                  and not k.startswith('_') \n",
    "                                                  and k != 'tmp' and k != 'In' and k != 'Out' \n",
    "                                                  and not hasattr(v, '__call__')]\n",
    "\n",
    "def keep_global_var(var_list):\n",
    "    tmp = globals().copy()\n",
    "    v = [k for k, v in tmp.items() if not k.startswith('_') \n",
    "                                      and not str(type(v)).endswith('module\\'>') \n",
    "                                      and k != 'tmp' and k != 'In' and k != 'Out' \n",
    "                                      and not hasattr(v, '__call__')]\n",
    "    for _v in v:\n",
    "        if _v not in var_list:\n",
    "            del globals()[_v]\n",
    "    \n",
    "\n",
    "# Make a list of calibration images\n",
    "camParams = util.CamParams()\n",
    "\n",
    "if 0:\n",
    "    images = glob.glob('../camera_cal/calibration*.jpg')\n",
    "    camParams.calibration(images, 6, 9)\n",
    "    camParams.save(0)\n",
    "else:\n",
    "    camParams.load(0)\n",
    "    \n",
    "\n",
    "mtx, dist, _, _ = camParams.get()\n",
    "img_dist = matplotlib.image.imread('../camera_cal/calibration1.jpg')\n",
    "img_undist = cv2.undistort(img_dist, mtx, dist, None, mtx)\n",
    "\n",
    "util.plot_comparison(img_dist, img_undist)\n",
    "\n",
    "# Only keep `camParams` in the python environment\n",
    "keep_global_var(['camParams']); printvars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Undistortion\n",
    "\n",
    "Run below cell to apply a distortion correction to raw images.\n",
    "> Notice that this step has been writen into `util.LaneDetector.preProcess()` and will be automatically invoked when `preProcess` runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn_img = '../test_images/straight_lines1.jpg'\n",
    "fn_img = '../test_images/test3.jpg'\n",
    "\n",
    "# Distorted Image\n",
    "img_rgb_dist = matplotlib.image.imread(fn_img)\n",
    "\n",
    "img_rgb = camParams.undistort(img_rgb_dist)\n",
    "\n",
    "# Convert to Gary for later use\n",
    "img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "util.plot_comparison(img_rgb_dist, img_rgb)\n",
    "\n",
    "keep_global_var(['camParams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Perpective Transform\n",
    "Four source points of a quadrangle should be identified for a perspective transform. In order to pick four points in a trapzoidal shape representing a rectangle I have to find a pair of parallel lane lines as straight as possible.\n",
    "\n",
    "I've provided `persTrans.get_y_from_x` and `persTrans.get_x_from_y` to help get the coordinates of the four points in the image coordinate. Fine-tune `pts1` and `pts2` to obtain the best perpective transform.\n",
    "\n",
    "> Don't forget to update `pts1` and `pts2` to `util.LaneDetector.loadCamParams()` after adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_img = camParams.undistort(matplotlib.image.imread('../test_images/straight_lines1.jpg'))\n",
    "persTrans = util.PersTrans()\n",
    "\n",
    "pts1 = np.float32([[204, 720], [1110, 720], [503, 515], [789, 515]])\n",
    "pts2 = np.float32([[204, 720], [1110, 720], [204, 300], [1110, 300]])\n",
    "\n",
    "print('y: %.1f'%persTrans.get_y_from_x(550, pts1[2], pts1[0]))\n",
    "print('x: %.1f'%persTrans.get_x_from_y(483, pts1[1], pts1[3]))\n",
    "\n",
    "# 源坐标，4个蓝色点\n",
    "pts1 = np.float32([[204, 720], [1110, 720], [550, 483], [738, 483]])\n",
    "# 转换后坐标，4个绿色小圈\n",
    "pts2 = np.float32([[300, 720], [940, 720], [300, 0], [940, 0]])\n",
    "\n",
    "# Compute perpective transform\n",
    "pers_M = persTrans.computeM(pts1, pts2, pers_img)\n",
    "\n",
    "# Apply perpective transform\n",
    "# `img.shape` is (rows, cols), here should be (cols, rows)\n",
    "pers_img_dst = cv2.warpPerspective(pers_img, pers_M, (1280, 720))\n",
    "\n",
    "util.plot_comparison(pers_img, pers_img_dst)\n",
    "\n",
    "keep_global_var(['camParams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some other points to see which one is most suitable for using in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_img_list = images = glob.glob('../test_images/my*')\n",
    "fn_img_list.insert(0, '../test_images/straight_lines1.jpg')\n",
    "persTrans = util.PersTrans()\n",
    "\n",
    "\"\"\"\n",
    "pts[0], 源坐标， 4个蓝色点\n",
    "pts[1], 转换后坐标， 4个绿色小圈\n",
    "pts[2], (rows, cols)\n",
    "\"\"\"\n",
    "pts_list = [[np.float32([[204, 720], [1110, 720], [503, 515], [789, 515]]),\n",
    "             np.float32([[204, 720], [1110, 720], [204, 300], [1110, 300]]),\n",
    "             (720, 1280)],\n",
    "            [np.float32([[204, 720], [1110, 720], [550, 483], [738, 483]]),\n",
    "             np.float32([[300, 720], [940, 720], [300, 0], [940, 0]]),\n",
    "             (720, 1280)],\n",
    "            [np.float32([[204, 720], [1110, 720], [550, 483], [738, 483]]),\n",
    "             np.float32([[320, 820], [900, 820], [320, 0], [900, 0]]),\n",
    "             (820, 1280)]]\n",
    "\n",
    "for fn_img in fn_img_list:\n",
    "    img = camParams.undistort(matplotlib.image.imread(fn_img))\n",
    "    img_disp_list = [img]\n",
    "    \n",
    "    for pts in pts_list:\n",
    "        # Compute perpective transform\n",
    "        pers_M = persTrans.computeM(pts[0], pts[1])\n",
    "    \n",
    "        # Apply perpective transform\n",
    "        # param shape `pts[2]` is (rows, cols), here should be (cols, rows)\n",
    "        img_pers = cv2.warpPerspective(img, pers_M, pts[2][1::-1])\n",
    "        img_disp_list.append(img_pers)\n",
    "        \n",
    "    util.plot_stack(img_disp_list, titles=[fn_img])\n",
    "    \n",
    "keep_global_var(['camParams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Edge Detection\n",
    "I've tried the edge detection method discussed in the class, and also changed the order of edge detection and perspective tranform to find which order is better. The comparison shows that the order of which one going first has little effect on the final result. Overall， edge detection going first will results in fewer noise, so I will do perspective transform after edge detection.\n",
    "\n",
    "However, from the result shown after running below cell, we can see that the original color and gradient algorithm does not work well under shadow environment such as \"myest0.png\" and \"mytest3.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_img_list = ['../test_images/mytest0.png',\n",
    "               '../test_images/mytest1.png',\n",
    "               '../test_images/mytest3.png']\n",
    "\n",
    "#fn_img_list.insert(0, '../test_images/straight_lines1.jpg')\n",
    "#fn_img_list.insert(0, '../test_images/test2.jpg')\n",
    "\n",
    "from util import EdgeDetector\n",
    "laneDetector = util.LaneDetector()\n",
    "laneDetector.loadCamParams(0)\n",
    "\n",
    "def detPossibleEdges(img_rgb, sob_x_thresh=(20, 100), sat_thresh=(170, 255), ret_temps=False):\n",
    "    \"\"\"\n",
    "    Detect Possible Edges\n",
    "    \"\"\"\n",
    "    # Convert to HLS color space and separate the V channel\n",
    "    hls = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HLS)\n",
    "    l_channel, s_channel = hls[:, :, 1], hls[:, :, 2]\n",
    "    \n",
    "    # Sobel x\n",
    "    sobelx_l = EdgeDetector.Sobel(l_channel, 1, 0)\n",
    "\n",
    "    # Threshold x gradient\n",
    "    sob_x_binary_l = EdgeDetector.BinaryThresholding(sobelx_l, sob_x_thresh)\n",
    "\n",
    "    # Threshold color channel\n",
    "    sat_binary = EdgeDetector.BinaryThresholding(s_channel, sat_thresh)\n",
    "\n",
    "    if ret_temps == True:\n",
    "        temps = {'l': l_channel, \n",
    "                 'lx': sobelx_l, \n",
    "                 'lb': sob_x_binary_l, \n",
    "                 's': s_channel, \n",
    "                 'sb': sat_binary}\n",
    "        return sob_x_binary_l | sat_binary, temps\n",
    "    else:\n",
    "        return sob_x_binary_l | sat_binary\n",
    "    \n",
    "\n",
    "for fn_img in fn_img_list:\n",
    "    # Read and undistort the image\n",
    "    img_rgb = cv2.cvtColor(cv2.imread(fn_img), cv2.COLOR_BGR2RGB)\n",
    "    img_rgb = laneDetector.preProcess(img_rgb)\n",
    "\n",
    "    # 全图Edge => Pers\n",
    "    img_edge, tmp = detPossibleEdges(img_rgb, ret_temps=True)\n",
    "\n",
    "    img_edge_pers = cv2.warpPerspective(img_edge, laneDetector.pers_M, laneDetector.pers_shape[1::-1])\n",
    "\n",
    "    temp = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,3));\n",
    "    img_edge_pers_denoised = cv2.morphologyEx(img_edge_pers, cv2.MORPH_OPEN, temp);\n",
    "    \n",
    "    # Pers => Edge\n",
    "    #img_pers = cv2.warpPerspective(img_rgb, laneDetector.pers_M, laneDetector.pers_shape[1::-1])\n",
    "    #img_pers_edge = laneDetector.detPossibleEdges(img_pers, ret_temps=True)\n",
    "    #img_pers_edge_denoised = cv2.morphologyEx(img_pers_edge, cv2.MORPH_OPEN, temp);\n",
    "\n",
    "    # Plot the result\n",
    "    util.plot_stack([tmp['l'], tmp['lx'], tmp['lb']], titles=['%s: l'%fn_img, 'lx', 'lb'])\n",
    "    util.plot_stack([tmp['s'], tmp['sb'], img_edge_pers_denoised], titles=['s', 'sb', 'img_edge_pers'])\n",
    "    #util.plot_stack([img_edge, img_edge_pers, img_edge_pers_denoised], \n",
    "    #                titles=['img_edge_roi', 'img_edge_pers', 'img_edge_pers_denoised'])\n",
    "    \n",
    "keep_global_var([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the code can work well in dark environments, instead of thresholding `s_channel` I apply an sobel along horizontal direction on `s_channel` which improve a lot on edge detection. You can pay special attention to \"myest0.png\" and \"mytest3.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_img_list = images = glob.glob('../test_images/my*')\n",
    "fn_img_list.insert(0, '../test_images/straight_lines1.jpg')\n",
    "fn_img_list.insert(1, '../test_images/test1.jpg')\n",
    "\n",
    "from util import EdgeDetector, LaneDetector\n",
    "laneDetector = util.LaneDetector()\n",
    "laneDetector.loadCamParams(0)\n",
    "\n",
    "def detPossibleEdges(img_rgb, sob_x_thresh=(20, 100), ret_temps=False):\n",
    "    \"\"\"\n",
    "    Detect Possible Edges\n",
    "    \"\"\"\n",
    "    # Convert to HLS color space and separate the V channel\n",
    "    hls = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HLS)\n",
    "    l_channel, s_channel = hls[:, :, 1], hls[:, :, 2]\n",
    "    \n",
    "    # Sobel x\n",
    "    sobelx_l = EdgeDetector.Sobel(l_channel, 1, 0)\n",
    "\n",
    "    # Threshold x gradient\n",
    "    sob_x_binary_l = EdgeDetector.BinaryThresholding(sobelx_l, sob_x_thresh)\n",
    "\n",
    "    # Threshold color channel    \n",
    "    # Sobel x on s_channel\n",
    "    sobelx_s = EdgeDetector.Sobel(s_channel, 1, 0)\n",
    "\n",
    "    # Threshold x gradient\n",
    "    sob_x_binary_s = EdgeDetector.BinaryThresholding(sobelx_s, sob_x_thresh)    \n",
    "    \n",
    "    if ret_temps == True:\n",
    "        temps = {'l': l_channel, \n",
    "                 'lx': sobelx_l, \n",
    "                 'lb': sob_x_binary_l, \n",
    "                 's': s_channel, \n",
    "                 'sx': sobelx_s,\n",
    "                 'sb': sob_x_binary_s}\n",
    "        return sob_x_binary_l | sob_x_binary_s, temps\n",
    "    else:\n",
    "        return sob_x_binary_l | sob_x_binary_s\n",
    "\n",
    "\n",
    "for fn_img in fn_img_list:\n",
    "    # Read and undistort the image\n",
    "    img_rgb = cv2.cvtColor(cv2.imread(fn_img), cv2.COLOR_BGR2RGB)\n",
    "    img_rgb = laneDetector.preProcess(img_rgb)\n",
    "\n",
    "    # 全图Edge => Pers\n",
    "    img_edge, tmp = detPossibleEdges(img_rgb, ret_temps=True)\n",
    "    LaneDetector.KeepROI(img_edge)\n",
    "\n",
    "    img_edge_pers = cv2.warpPerspective(img_edge, laneDetector.pers_M, laneDetector.pers_shape[1::-1])\n",
    "\n",
    "    temp = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,3));\n",
    "    img_edge_pers_denoised = cv2.morphologyEx(img_edge_pers, cv2.MORPH_OPEN, temp);\n",
    "    \n",
    "    # Pers => Edge\n",
    "#     img_pers = cv2.warpPerspective(img_rgb, laneDetector.pers_M, laneDetector.pers_shape[1::-1])\n",
    "#     img_pers_edge = laneDetector.detPossibleEdges(img_pers)\n",
    "#     img_pers_edge_denoised = cv2.morphologyEx(img_pers_edge, cv2.MORPH_OPEN, temp);\n",
    "#     util.plot_stack([img_pers, img_pers_edge, img_pers_edge_denoised], \n",
    "#                     titles=['img_pers', 'img_pers_edge', 'img_pers_edge_denoised'])\n",
    "    \n",
    "    # Plot the result\n",
    "    util.plot_stack([tmp['l'], tmp['lx'], tmp['lb']], titles=['%s: l'%fn_img, 'lx', 'lb'])\n",
    "    util.plot_stack([tmp['s'], tmp['sx'], tmp['sb']], titles=['s', 'sx', 'sb'])\n",
    "    util.plot_stack([img_edge, img_edge_pers, img_edge_pers_denoised], \n",
    "                    titles=['img_edge_roi', 'img_edge_pers', 'img_edge_pers_denoised'])\n",
    "\n",
    "keep_global_var([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finding the Lines: \n",
    "\n",
    "---\n",
    "#### Histogram Peaks\n",
    "There is no big change here with class solution, but I take the edge width into account when I compute the histogram. Instead of summing up all pixels along each column, I sum up the pixels inside adjacent columns which I set the width to 40, see `DetEdgePeaks(binary_edges, edge_width=40)`. Thus, many small edges will be dropped as a lane line should have a certain width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import EdgeDetector, LaneDetector\n",
    "\n",
    "fn_img_list = images = glob.glob('../test_images/*')\n",
    "#fn_img_list.insert(0, '../test_images/straight_lines1.jpg')\n",
    "#fn_img_list.append('../test_images/test1.jpg')\n",
    "\n",
    "laneDetector = LaneDetector()\n",
    "laneDetector.loadCamParams(0)\n",
    "\n",
    "\n",
    "def PreProcess(fn_img):\n",
    "    # Read and undistort the image\n",
    "    img_rgb = cv2.cvtColor(cv2.imread(fn_img), cv2.COLOR_BGR2RGB)\n",
    "    img_rgb = laneDetector.preProcess(img_rgb)\n",
    "\n",
    "    # 全图Edge => Pers\n",
    "    img_edge = LaneDetector.detPossibleEdges(img_rgb)\n",
    "    LaneDetector.KeepROI(img_edge)\n",
    "\n",
    "    img_edge_pers = cv2.warpPerspective(img_edge, laneDetector.pers_M, laneDetector.pers_shape[1::-1])\n",
    "\n",
    "    temp = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,3));\n",
    "    img_edge_pers_denoised = cv2.morphologyEx(img_edge_pers, cv2.MORPH_OPEN, temp);\n",
    "    \n",
    "    return img_edge_pers_denoised\n",
    "\n",
    "\n",
    "def DetEdgePeaks(binary_edges, edge_width=40):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_edges[binary_edges.shape[0]//2:,:], axis=0)\n",
    "    #histogram = np.sum(binary_edges, axis=0)\n",
    "\n",
    "    # Compute histogram within continuous columns, determined by `edge_width`\n",
    "    histogram = np.array([np.sum(histogram[i:i+edge_width])\n",
    "                          for i in range(len(histogram))])\n",
    "    for i in reversed(range(1, edge_width)):\n",
    "        histogram[-i] += histogram[-1]*(edge_width-i)\n",
    "\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    return leftx_base, rightx_base, histogram/edge_width\n",
    "\n",
    "\n",
    "for fn_img in fn_img_list:\n",
    "    bimg_edge = PreProcess(fn_img)\n",
    "    \n",
    "    # Create histogram of image binary activations\n",
    "    leftx_base, rightx_base, histogram = DetEdgePeaks(bimg_edge)\n",
    "\n",
    "    leftxy, rightxy, out_img = LaneDetector.FindLanePixels(bimg_edge, leftx_base, rightx_base)\n",
    "    \n",
    "    # Plot the result\n",
    "    util.plot_stack([bimg_edge, out_img, out_img])\n",
    "    # Visualize the resulting histogram\n",
    "    plt.cla(); plt.plot(histogram); \n",
    "    axes = plt.gca(); axes.set_xlim([0, 1280]); axes.set_ylim([0, 720])\n",
    "\n",
    "keep_global_var([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding windows and Fit Polynomial\n",
    "As shown in the previous animation, I have got two highest peaks from the histogram as a starting point for the determining where the lane lines are. Now I will use sliding windows moving upward in the image to determine where the lane lines go. Notice that in my solution the movement trend of sliding windows is used to predict the center of new sliding window, which is very important to track a curved lane line.\n",
    "\n",
    "In order to save time and utilize the characteristics that lane lines appear at a close position on the two frames, a margin around the previous line position is the first target to search. **However, there is the necessity for checking similarity and confidence of previous lane lines, and switch back to whole region resarch if fail to meet the requirement.**\n",
    "\n",
    "---\n",
    "### Measuring Curvature\n",
    "$x = A'y^2 + B'y + C$\n",
    "\n",
    "o 代表 offset，由于计算 erpective transform 时候引起的\n",
    "\n",
    "$m(x+o) = A(ny)^2 + B(ny) + C$  \n",
    "$mx = (An^2)y^2 + (Bn)y + (C-mo)$  \n",
    "$x = (An^2/m)y^2 + (Bn/m)y + (C/m-o)$  \n",
    "\n",
    "\n",
    "$$\\begin{cases}\n",
    "An^2/m = A' \\\\\n",
    "Bn/m = B' \\\\\n",
    "C/m - o = C'\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "进一步得到\n",
    "  \n",
    "$$\\begin{cases}\n",
    "A = A'm/n^2 \\\\\n",
    "B = B'm/n \\\\\n",
    "C = C'm + mo\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "---\n",
    "## Whole Processing of an Image or a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "%reload_ext autoreload\n",
    "%aimport util\n",
    "%autoreload 1 \n",
    "%matplotlib inline\n",
    "np.set_printoptions(2)\n",
    "\n",
    "LaneDetector = util.LaneDetector # Equal to \"from util import LaneDetector\"\n",
    "\n",
    "laneDetector = LaneDetector()\n",
    "laneDetector.loadCamParams(0)\n",
    "\n",
    "def Overlay_Lane(img_src, coef_l, coef_r, pers_shape, pers_M_inv, beta=0.2, alpha=1):\n",
    "    \"\"\"\n",
    "    @alpha – weight of the `img_src` array elements.\n",
    "    @beta – weight of the lane layer array elements.\n",
    "    \"\"\"\n",
    "    ploty = np.linspace(0, pers_shape[0]-1, pers_shape[0])\n",
    "    left_fitx = coef_l[0]*ploty**2 + coef_l[1]*ploty + coef_l[2] if coef_l != None else []\n",
    "    right_fitx = coef_r[0]*ploty**2 + coef_r[1]*ploty + coef_r[2]  if coef_r != None else []\n",
    "    \n",
    "    pers_shape = list(pers_shape); pers_shape.append(3)\n",
    "    img_lane = np.zeros(pers_shape, img_src.dtype)\n",
    "    \n",
    "    if len(left_fitx) == len(right_fitx):\n",
    "        for xl, xr, y in zip(left_fitx.astype(np.int), \n",
    "                             right_fitx.astype(np.int), \n",
    "                             ploty.astype(np.int)):\n",
    "            cv2.line(img_lane, (xl, y), (xr, y), (0, 255, 0), 1)\n",
    "\n",
    "    img_lane_warpinv = cv2.warpPerspective(img_lane, pers_M_inv, img_src.shape[1::-1])\n",
    "    \n",
    "    #util.plot_stack(img_lane_warpinv)\n",
    "    \n",
    "    return cv2.addWeighted(img_src, alpha, img_lane_warpinv, beta, gamma=0)\n",
    "     \n",
    "\n",
    "def FrameProcess(img_rgb, track_similarity=90, track_conf=50, show_model=0):\n",
    "    return_slidewindows = True if show_model == 1 else False\n",
    "    \n",
    "    # Apply undistortion\n",
    "    img_rgb = laneDetector.preProcess(img_rgb)\n",
    "\n",
    "    binary_edges = LaneDetector.detPossibleEdges(img_rgb)\n",
    "    LaneDetector.KeepROI(binary_edges)\n",
    "  \n",
    "    # Apply perpective transform, `img.shape` lies as (rows, cols), here should be (cols, rows)\n",
    "    bin_edge_pers = cv2.warpPerspective(binary_edges, laneDetector.pers_M, (1280, 720)) \n",
    "    \n",
    "    temp = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,3));\n",
    "    bin_edge_pers_denoised = cv2.morphologyEx(bin_edge_pers, cv2.MORPH_OPEN, temp);\n",
    "    \n",
    "    # Try to use the result from last frame\n",
    "    left_fit, right_fit = laneDetector.getLaneHistory()\n",
    "    conf, similarity = laneDetector.ComputeConf(left_fit, right_fit)\n",
    "    \n",
    "    if (similarity < track_similarity) and (conf > track_conf):\n",
    "        left_xy, right_xy, img_lane_windows = laneDetector.ReloadLanePixels(bin_edge_pers_denoised, \n",
    "                                                                            left_fit,\n",
    "                                                                            right_fit,\n",
    "                                                                            return_result=return_slidewindows)\n",
    "        left_fit, right_fit = LaneDetector.FitPolynomial(left_xy, right_xy)\n",
    "    else:\n",
    "        left_fit, right_fit = None, None\n",
    "        \n",
    "    # Check if need to research again\n",
    "    if left_fit == None and right_fit == None:\n",
    "        #print('failed')\n",
    "        leftx_base, rightx_base = LaneDetector.detEdgePeaks(bin_edge_pers_denoised, edge_width=40)\n",
    "        # Find our lane pixels first\n",
    "        left_xy, right_xy, img_lane_windows = LaneDetector.FindLanePixels(bin_edge_pers_denoised, \n",
    "                                                                          leftx_base,\n",
    "                                                                          rightx_base,\n",
    "                                                                          return_result=return_slidewindows)\n",
    "        left_fit, right_fit = LaneDetector.FitPolynomial(left_xy, right_xy)\n",
    "        \n",
    "    laneDetector.pushLane([left_fit, right_fit])\n",
    "    img_overlay = Overlay_Lane(img_rgb, left_fit, right_fit, laneDetector.pers_shape, laneDetector.pers_M_inv)\n",
    "    \n",
    "#     tmp_left_xy = left_xy.copy() \n",
    "#     tmp_left_xy[0] = np.array(tmp_left_xy[0]) - 96\n",
    "#     tmp_right_xy = right_xy.copy() \n",
    "#     tmp_right_xy[0] = np.array(tmp_right_xy[0]) + 170\n",
    "    \n",
    "#     tmp_left_fit, tmp_right_fit = LaneDetector.FitPolynomial(tmp_left_xy, tmp_right_xy)\n",
    "    left_curverad, right_curverad = LaneDetector.MeasureCurvature(left_fit, right_fit,\n",
    "                                                                  bin_edge_pers_denoised.shape[0]-1)\n",
    "    center_offset = LaneDetector.MeausreVehicelCenter(left_fit, right_fit, bin_edge_pers_denoised.shape)\n",
    "    \n",
    "    cv2.putText(img_overlay, \"radius: %dm\"%(int(left_curverad)), (50, 100),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, [255, 128, 0], thickness=3)\n",
    "    cv2.putText(img_overlay, \"offset: %.2fm\"%(center_offset), (50, 150),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, [255, 128, 0], thickness=3)  \n",
    "    if show_model == 1:\n",
    "        L1_1 = cv2.resize(img_overlay, None, fx=0.5, fy=0.5)\n",
    "        conf, similarity = laneDetector.ComputeConf(left_fit, right_fit)\n",
    "        cv2.putText(L1_1, \"s, c: %d, %d\"%(int(similarity), int(conf)), (350, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, [255, 128, 0], thickness=3)\n",
    "        L1_2 = cv2.cvtColor(cv2.resize(binary_edges*255, None, fx=0.5, fy=0.5), cv2.COLOR_GRAY2RGB)\n",
    "        L2_1 = cv2.cvtColor(cv2.resize(bin_edge_pers*255, None, fx=0.5, fy=0.5), cv2.COLOR_GRAY2RGB)\n",
    "        L2_2 = cv2.resize(img_lane_windows, None, fx=0.5, fy=0.5)\n",
    "\n",
    "        img_ret = np.vstack((np.hstack((L1_1, L1_2)), np.hstack((L2_1, L2_2))))\n",
    "    else:\n",
    "        img_ret = img_overlay\n",
    "    return img_ret \n",
    "\n",
    "\n",
    "fn_img = '../test_images/test1.jpg'\n",
    "img_rgb = cv2.cvtColor(cv2.imread(fn_img), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "%time img_res = FrameProcess(img_rgb, track_similarity=90, track_conf=50, show_model=1)\n",
    "util.plot_stack(img_res)\n",
    "\n",
    "%time img_res = FrameProcess(img_rgb, track_similarity=90, track_conf=50, show_model=1)\n",
    "util.plot_stack(img_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "def callback(img):\n",
    "    img = FrameProcess(img, track_similarity=90, track_conf=50, show_model=1)\n",
    "    return img\n",
    "   \n",
    "input_video = \"../project_video.mp4\" ; white_output = 'output/0.mp4'; \n",
    "#input_video = \"../challenge_video.mp4\" ; white_output = 'output/1.mp4'; \n",
    "#input_video = \"../harder_challenge_video.mp4\" ; white_output = './output/2.mp4'; \n",
    "\n",
    "# To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "# To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "# Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "# You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip1 = VideoFileClip(input_video)\n",
    "# NOTE: this function expects color images!!\n",
    "white_clip = clip1.fl_image(callback).subclip(0, 5)\n",
    "#white_clip = clip1.fl_image(callback)\n",
    "\n",
    "\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "def callback(img):\n",
    "    img = FrameProcess(img, track_similarity=90, track_conf=50, show_model=0)\n",
    "    return img\n",
    "   \n",
    "input_videos = []\n",
    "input_videos.append(\"../project_video.mp4\" )\n",
    "input_videos.append(\"../challenge_video.mp4\")\n",
    "input_videos.append(\"../harder_challenge_video.mp4\")\n",
    "\n",
    "\n",
    "for input_video in input_videos:\n",
    "    clip = VideoFileClip(input_video)\n",
    "    white_clip = clip.fl_image(callback)\n",
    "    base = os.path.basename(input_video)\n",
    "    white_clip.write_videofile('output/'+base, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
